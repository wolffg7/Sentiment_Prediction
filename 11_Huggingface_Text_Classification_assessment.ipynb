{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolffg7/Sentiment_Prediction/blob/main/11_Huggingface_Text_Classification_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htO7JShhI4sa"
      },
      "source": [
        "## Assessment: Classifying authors with DistilBERT\n",
        "Please use the HuggingFace Transformer library for author classification:\n",
        "* use the DistilBERT architecture this time,\n",
        "* collect data from 4 authors,\n",
        "* create, train and evaluate a classifier with DistilBERT and 4 authors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsdBQBpDAX5X"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tensorflow transformers\n",
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wew63ncSAmvw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import activations, optimizers, losses\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import pickle\n",
        "import spacy\n",
        "from urllib.request import urlretrieve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1uUKILjBqq1"
      },
      "source": [
        "## 1. Exercise: Dataset acquisition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def sentencesSplit(raw_text):\n",
        "  doc = nlp(raw_text)\n",
        "  sentences = [sent.text for sent in doc.sents]\n",
        "  return sentences\n"
      ],
      "metadata": {
        "id": "p9fAu51wXiW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book1=\"http://www.gutenberg.org/files/2151/2151-0.txt\"\n",
        "book2=\"https://www.gutenberg.org/cache/epub/70589/pg70589.txt\"\n",
        "book3=<TODO>\n",
        "book4=<TODO>\n",
        "urlretrieve(book1, 'book1.txt')\n",
        "urlretrieve(book2, 'book2.txt')\n",
        "urlretrieve(<TODO>)\n",
        "urlretrieve(<TODO>)\n",
        "book1_text = open(\"book1.txt\", encoding='utf-8').read().lower()\n",
        "book2_text = open(\"book2.txt\", encoding='utf-8').read().lower()\n",
        "book3_text = <TODO>\n",
        "book4_text = <TODO>"
      ],
      "metadata": {
        "id": "oyYxDTArW8us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book1_sents = sentencesSplit(book1_text)\n",
        "book2_sents = sentencesSplit(book2_text)\n",
        "book3_sents = <TODO>\n",
        "book3_sents = <TODO>"
      ],
      "metadata": {
        "id": "4z_e_aSvX1_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 0.8\n",
        "t_split_1  = int(TRAIN_SIZE*len(book1_sents))\n",
        "t_split_2  = int(TRAIN_SIZE*len(book2_sents))\n",
        "t_split_3  = <TODO>\n",
        "t_split_4  = <TODO>"
      ],
      "metadata": {
        "id": "Xp4SGIWO1kBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = book1_sents[:t_split_1] + book2_sents[:t_split_2] + <TODO...>\n",
        "Y_train = np.concatenate((np.zeros((t_split_1)), np.ones((t_split_2), <TODO...> ))) \n",
        "\n",
        "X_test  = book1_sents[t_split_1:] + book2_sents[t_split_2:] + <TODO...>\n",
        "Y_test  = np.concatenate((np.zeros((len(book1_sents)-t_split_1)), np.ones((len(book2_sents)-t_split_2), <TODO...)))"
      ],
      "metadata": {
        "id": "2_m0goznY-gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v06X47TXFFup"
      },
      "source": [
        "# 2. Exercise: Data preparation\n",
        "\n",
        "We are using [DisitlBERT](https://huggingface.co/transformers/model_doc/distilbert.html) model, in which we have to convert each sentence into a tuple containing the text's [input ids](https://huggingface.co/transformers/glossary.html#input-ids) and the corresponding [attention masks](https://huggingface.co/transformers/glossary.html#attention-mask). In order to do so, the corresponding tokizer can be used: [DistilBertTokenizer](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer)\n",
        "\n",
        "We trim or pad the processed sentences to MAX_LEN (due to `truncation=True` and `padding=True` parameters) - this is an easy solution to handle inputs with different lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3NrIFRoBR0z"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = <TODO>\n",
        "MAX_LEN = <TODO>\n",
        "\n",
        "sample = X_train[100]\n",
        "\n",
        "tokenizer = <TODO>.from_pretrained(MODEL_NAME)\n",
        "inputs = tokenizer(<TODO>)\n",
        "\n",
        "print(f'sentence: \\'{sample}\\'')\n",
        "print(f'input ids: {inputs[\"input_ids\"]}')\n",
        "print(f'attention mask: {inputs[\"attention_mask\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMgiD2cgATjr"
      },
      "source": [
        "This transformation must be applied to each sentence in the corpus. Here's how we do it: `construct_encodings` maps the tokenizer to each `sentence` and aggregates them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycf5U8fj6ocz"
      },
      "outputs": [],
      "source": [
        "def construct_encodings(x, tokenizer, max_len, trucation=True, padding=True):\n",
        "    return tokenizer(x, max_length=max_len, truncation=trucation, padding=padding)\n",
        "    \n",
        "encodings_train = construct_encodings(X_train, tokenizer, max_len=MAX_LEN)\n",
        "encodings_test  = construct_encodings(X_test, tokenizer, max_len=MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbSYjFOlOut5"
      },
      "source": [
        "Now we have to convert the `encodings` and `y` (which holds the classes of the authors) into a [Tensorflow Dataset object](https://www.tensorflow.org/api_docs/python/tf/data/Dataset):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyOQIPhX6ocz"
      },
      "outputs": [],
      "source": [
        "def construct_tfdataset(encodings, y=None):\n",
        "    if y is not None:\n",
        "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
        "    else:\n",
        "        # this case is used when making predictions on unseen samples after training\n",
        "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
        "    \n",
        "tfdataset_train = construct_tfdataset(encodings_train, Y_train)\n",
        "tfdataset_test  = construct_tfdataset(encodings_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUo15LYpPqI1"
      },
      "source": [
        "Shuffle the training data and create batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKTJqUF5R-o4"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "tfdataset_train = tfdataset_train.shuffle(len(X_train))\n",
        "tfdataset_train = tfdataset_train.take(len(X_train))\n",
        "tfdataset_test = tfdataset_test.take(len(X_test))\n",
        "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
        "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nqs83LVFUQE"
      },
      "source": [
        "Our data is finally ready. Now we can do the fun part: model fitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Hk-8fDR_Pg"
      },
      "source": [
        "# 3. Exercise: Fine-tuning the model\n",
        "\n",
        "Fine-tuning the model is as easy as instantiating a model instance, [optimizer](https://developers.google.com/machine-learning/glossary#optimizer), and [loss](https://developers.google.com/machine-learning/glossary#loss), and then compiling/fitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVU_LoASQMcp"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 2\n",
        "\n",
        "model = <TODO>.from_pretrained(<TODO AND SET MULTIPLE CLASSES>)\n",
        "optimizer = <TODO>\n",
        "loss = <TODO>\n",
        "model.compile(<TODO>)\n",
        "\n",
        "model.fit(<TODO>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgz5Jfi9cmDv"
      },
      "source": [
        "## 4. Exercise: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBYlX43STjhl"
      },
      "source": [
        "Let's test our model with the test set and the default classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhmogfJhco0T"
      },
      "outputs": [],
      "source": [
        "print(model.evaluate(<TODO>))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(<TODO>)\n",
        "preds_classes = np.argmax(<TODO>)"
      ],
      "metadata": {
        "id": "rktYMdbT5IHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(<TODO>))"
      ],
      "metadata": {
        "id": "hOmlbMFe5edo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf=confusion_matrix(<TODO>)\n",
        "sns.heatmap(conf, annot=True, fmt='d')"
      ],
      "metadata": {
        "id": "NMBfI5ph5kEY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}